---
  - name: Initialize the Kubernetes control plane
    ansible.builtin.command: >
        kubeadm init \
        --cri-socket "{{ DataGatewayK8Cluster.cri_socket }}" \
        --control-plane-endpoint "{{ DataGatewayK8Cluster.control_plane_endpoint }}" \
        --pod-network-cidr "{{ DataGatewayK8Cluster.pod_network_cidr }}"
    register: kubeadm_init
    when: node is defined and node == "controller"

  - name: Create .kube directory in home
    ansible.builtin.file:
        path: "{{ ansible_env.HOME }}/.kube"
        state: directory

  - name: Copy admin.conf to .kube/config
    ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: "{{ ansible_env.HOME }}/.kube/config"
        owner: "{{ ansible_env.USER }}"
        group: "{{ ansible_env.USER }}"
        mode: '0644'
        remote_src: yes

  - name: Change ownership of .kube/config
    ansible.builtin.file:
        path: "{{ ansible_env.HOME }}/.kube/config"
        owner: "{{ ansible_env.USER }}"
        group: "{{ ansible_env.USER }}"
        mode: '0644'

  - name: Copy flannel.yaml from local to remote
    ansible.builtin.copy:
        src: templates/kube-flannel.yml
        dest: "/etc/kubernetes/kube-flannel.yml"
        owner: "{{ ansible_env.USER }}"
        group: "{{ ansible_env.USER }}"
        mode: '0644'
        remote_src: no

  - name: Add KUBECONFIG export to bash profile
    ansible.builtin.lineinfile:
        path: ~/.bash_profile
        line: 'export KUBECONFIG=/etc/kubernetes/admin.conf'
        create: yes
        insertafter: EOF

# NOTE: 2024-07-08: Downloading the kube-flannel.yml file has been very unreliable, so just checked it into the repo.
  # https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
  - name: Apply Flannel network
    ansible.builtin.command: kubectl apply -f /etc/kubernetes/kube-flannel.yml
    when: node is defined and node == "controller"

    # Setup local registry to store Docker images to deploy to worker nodes.
    # I'm doing it this way to save on AWS ECR Egress costs. $0.10/GB O_o...
    # https://distribution.github.io/distribution/about/deploying/
    # And because I don't have a way for letsencrypt to validate an internal only DNS, I'm going to use self signed certs
    # https://distribution.github.io/distribution/about/insecure/

  - name: Set docker cert directory
    set_fact:
        docker_cert_dir: "/etc/docker/certs.d/{{ DataGatewayK8Cluster.control_plane_endpoint }}/"

  - name: Create certs directory
    file:
        path: "{{ docker_cert_dir }}"
        state: directory

  - name: Generate an OpenSSL private key
    community.crypto.openssl_privatekey:
        path: "{{ docker_cert_dir }}/domain.key"
        size: 4096
        type: RSA

  - name: Generate a self-signed certificate
    community.crypto.x509_certificate:
        path: "{{ docker_cert_dir }}/domain.crt"
        privatekey_path: "{{ docker_cert_dir }}/domain.key"
        subject:
            commonName: "{{ DataGatewayK8Cluster.control_plane_endpoint }}"
        provider: selfsigned
        selfsigned_not_after: +365d
        selfsigned_not_before: +0s
        subject_alt_name:
            - DNS:"{{ DataGatewayK8Cluster.control_plane_endpoint }}"

    # NOTE: The above certs are going to be stored in hashicorp vault at the end of this playbook and deployed to worker nodes subsequently.

  - name: Start a local Docker image registry with TLS
    docker_container:
        name: registry
        image: registry:latest
        state: started
        restart_policy: always
        ports:
            - "5000:5000"
        volumes:
            - "{{ docker_cert_dir }}:/certs"
        env:
            REGISTRY_HTTP_ADDR: "0.0.0.0:5000"
            REGISTRY_HTTP_TLS_CERTIFICATE: "/certs/domain.crt"
            REGISTRY_HTTP_TLS_KEY: "/certs/domain.key"
        container_default_behavior: no_defaults
        
  # I didn't think it was worth the memory to spin up a VM just for HAProxy, so I'm bundling it with the control plane.
  - name: Install HA Proxy to load balance all the worker node ports
    package:
        name: haproxy
        state: present

# HA proxy will be configured in each service's respective playbook, we're just installing the basic package here.

  - name: Store secrets in Hashicorp Vault
    include_tasks: store_control_plane_secrets.yml
